{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fd995-fee9-4198-b50a-5fb3e2b61bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a78cb-e6f9-4165-94ba-3f1596a5d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csqa = load_dataset(\"tau/commonsense_qa\")\n",
    "csqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d74c74b-48f7-4594-ace3-301608865edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTERS = [\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
    "MAX_LEN = 576\n",
    "\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"You are a helpful reasoning assistant.\\n\"\n",
    "    \"Answer the multiple-choice question by outputting just one capital letter from {{A, B, C, D, E}}.\\n\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    "    \"Options:\\n{options}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "def format_options(choice_texts):\n",
    "    return \"\\n\".join([f\" {LETTERS[i]}) {t}\" for i,t in enumerate(choice_texts)])\n",
    "\n",
    "def build_prompt(example):\n",
    "    return PROMPT_TEMPLATE.format(\n",
    "        question=example[\"question\"],\n",
    "        options=format_options(example[\"choices\"][\"text\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1cad7e-b1dc-4363-9e17-209e0695e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i in random.sample(range(len(csqa[\"train\"])), 3):\n",
    "    e = csqa[\"train\"][i]\n",
    "    print(build_prompt(e))\n",
    "    print(\"GOLD:\", e[\"answerKey\"])\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cae506a-68c6-4da2-9d1d-469978e24507",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\", use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f914b7-ffcd-4468-bf89-c5312a1cc29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_item(ex, tok, max_len=MAX_LEN):\n",
    "    # 1) Build the text prompt the model will read\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        question=ex[\"question\"],\n",
    "        options=format_options(ex[\"choices\"][\"text\"])\n",
    "    )\n",
    "\n",
    "    # 2) Tokenize the prompt (the \"inputs\")\n",
    "    enc = tok(prompt, truncation=True, max_length=max_len, add_special_tokens=True)\n",
    "\n",
    "    # 3) Build the target we want the model to generate right after \"Answer:\"\n",
    "    #    We include a newline so decoding looks neat; it’s okay if the model emits just the letter.\n",
    "    gold_letter = ex[\"answerKey\"]                # 'A'..'E'\n",
    "    target_ids  = tok(gold_letter + \"\\n\", add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    # 4) Ensure prompt + target fits MAX_LEN (trim from the left if needed)\n",
    "    spill = len(enc[\"input_ids\"]) + len(target_ids) - max_len\n",
    "    # print(\"spill\", spill)\n",
    "    if spill > 0:\n",
    "        enc[\"input_ids\"]      = enc[\"input_ids\"][spill:]\n",
    "        enc[\"attention_mask\"] = enc[\"attention_mask\"][spill:]\n",
    "\n",
    "    # 5) Final input = prompt tokens + answer tokens\n",
    "    input_ids     = enc[\"input_ids\"] + target_ids\n",
    "    attention     = enc[\"attention_mask\"] + [1] * len(target_ids)\n",
    "\n",
    "    # 6) Labels: ignore loss on the prompt (-100), supervise only the answer tokens\n",
    "    labels = [-100] * len(input_ids)\n",
    "    start  = len(enc[\"input_ids\"])  # answer starts right after the prompt\n",
    "    for i, t in enumerate(target_ids):\n",
    "        labels[start + i] = t\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention,\n",
    "        \"labels\": labels,\n",
    "        \"answer_letter\": gold_letter,  # helpful for debugging/metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c922713-5ae4-4b83-a4dd-65736f9deaf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218aadfe-52be-49ae-811f-c816cdef9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_cols = csqa[\"train\"].column_names  # drop original text after mapping\n",
    "\n",
    "train_ds = csqa[\"train\"].map(lambda ex: build_item(ex, tok, MAX_LEN),\n",
    "                             remove_columns=remove_cols)\n",
    "val_ds   = csqa[\"validation\"].map(lambda ex: build_item(ex, tok, MAX_LEN),\n",
    "                                  remove_columns=remove_cols)\n",
    "test_ds  = csqa[\"test\"].map(lambda ex: build_item(ex, tok, MAX_LEN),\n",
    "                            remove_columns=remove_cols)\n",
    "\n",
    "train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b59538a-314f-4ff5-a6f9-4dbdca34140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __init__(self, tok):\n",
    "        self.tok = tok\n",
    "\n",
    "    def __call__(self, feats):\n",
    "        ids  = [f[\"input_ids\"] for f in feats]\n",
    "        attn = [f[\"attention_mask\"] for f in feats]\n",
    "        labs = [f[\"labels\"] for f in feats]\n",
    "\n",
    "        # Pad both together (tokenizer.pad wants input_ids present)\n",
    "        padded = self.tok.pad(\n",
    "            {\"input_ids\": ids, \"attention_mask\": attn},\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        batch_ids  = padded[\"input_ids\"]\n",
    "        batch_attn = padded[\"attention_mask\"]\n",
    "\n",
    "        # Pad labels to the same sequence length with -100\n",
    "        L = batch_ids.size(1)\n",
    "        batch_labs = torch.full((len(labs), L), -100, dtype=torch.long)\n",
    "        for i, lab in enumerate(labs):\n",
    "            batch_labs[i, :len(lab)] = torch.tensor(lab, dtype=torch.long)\n",
    "\n",
    "        return {\"input_ids\": batch_ids, \"attention_mask\": batch_attn, \"labels\": batch_labs}\n",
    "\n",
    "    \n",
    "collator = Collator(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b30dd-d7cb-419b-a6c3-fadb5720590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,   # load weights in bf16\n",
    "    device_map=\"auto\",            # place layers on GPU automatically\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"Loaded base model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9cef69-0af7-4bcb-b5a0-297bcb343ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base.gradient_checkpointing_enable()\n",
    "base.config.use_cache = False\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "        \"gate_proj\",\"up_proj\",\"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(base, lora_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eff465-e7ef-4b7b-8378-7f5049b8fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity: count trainable params\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,} / {total:,} ({100*trainable/total:.3f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a86d64-095c-4d50-aa56-7c70760e8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) print a few trainable names to see LoRA layers\n",
    "shown = 0\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(\"TRAINABLE:\", n, p.shape)\n",
    "        shown += 1\n",
    "    if shown >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f087ba8d-7b75-479a-a15f-f72424aa6a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ba614-6c57-4a56-9d27-74750d3c35bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afbab57a-2051-4bc3-87a2-047e0cd41447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred          # logits: [B, T, V], labels: [B, T]\n",
    "    logits = torch.tensor(logits)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    mask = labels.ne(-100)              # supervised positions (answer tokens at the end)\n",
    "    # index of the FIRST supervised token (start of the answer)\n",
    "    first_idx = mask.int().argmax(dim=1)  # shape [B]\n",
    "\n",
    "    rows = torch.arange(labels.size(0))\n",
    "    # target is the first answer token id\n",
    "    tgt  = labels[rows, first_idx]\n",
    "\n",
    "    # causal shift: logits at t predict token at t+1\n",
    "    # so we need the logits at (first_idx - 1) to predict the first answer token\n",
    "    pred_pos = first_idx - 1\n",
    "\n",
    "    # (safety) if any pred_pos < 0 (shouldn't happen with our construction), skip them\n",
    "    valid = pred_pos.ge(0)\n",
    "    if valid.sum() == 0:\n",
    "        return {\"accuracy\": 0.0}\n",
    "\n",
    "    rows_v = rows[valid]\n",
    "    pred_v = pred_pos[valid]\n",
    "    tgt_v  = tgt[valid]\n",
    "\n",
    "    pred = logits[rows_v, pred_v, :].argmax(dim=-1)\n",
    "    acc = (pred == tgt_v).float().mean().item()\n",
    "    return {\"accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f33724c-a739-4446-87f7-b98c3d3eedaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c01113b-ba30-408c-8872-cc10d615fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.eval  = lambda: None   # <-- add this\n",
    "\n",
    "# eval_metrics = trainer.evaluate()\n",
    "# print(eval_metrics)  # should include 'eval_accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb231dd-a29a-403f-bfe7-40f5576f4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer over *only* LoRA trainable params\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=2e-4, betas=(0.9, 0.999), weight_decay=0.0)\n",
    "optimizer.train = lambda: None   # accelerate compatibility\n",
    "optimizer.eval  = lambda: None\n",
    "\n",
    "full_args = TrainingArguments(\n",
    "    output_dir=\"./csqa_llama32_full\",\n",
    "    num_train_epochs=2,                   # start with 2; you can try 3 later\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size  = 8,\n",
    "    gradient_accumulation_steps = 2,  # effective batch 8 “updates”\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    bf16=True, fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=[\"none\"],\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "full_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=full_args,\n",
    "    train_dataset=train_ds,   # full train split (from Step 4C)\n",
    "    eval_dataset=val_ds,      # full validation split\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,  # the fixed, “causal shift” version\n",
    "    optimizers=(optimizer, None),\n",
    ")\n",
    "\n",
    "full_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5374212-d304-4b12-bf66-87a5527a8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def manual_eval_accuracy(model, ds, collator, batch_size=2):  # small batch size helps\n",
    "    model.eval()\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
    "    device = next(model.parameters()).device\n",
    "    total = correct = 0\n",
    "    loss_sum = 0.0\n",
    "    nloss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "            out = model(**batch)  # logits [B,T,V], loss averaged over supervised tokens\n",
    "        logits, labels = out.logits, batch[\"labels\"]\n",
    "\n",
    "        mask = labels.ne(-100)\n",
    "        first_idx = mask.int().argmax(dim=1)      # start of answer\n",
    "        rows = torch.arange(labels.size(0), device=device)\n",
    "        tgt = labels[rows, first_idx]\n",
    "        pred_pos = first_idx - 1                   # causal shift\n",
    "        valid = pred_pos.ge(0)\n",
    "\n",
    "        if valid.any():\n",
    "            rows_v = rows[valid]\n",
    "            pred_v = pred_pos[valid]\n",
    "            tgt_v  = tgt[valid]\n",
    "            pred   = logits[rows_v, pred_v, :].argmax(dim=-1)\n",
    "            correct += (pred == tgt_v).sum().item()\n",
    "            total   += valid.sum().item()\n",
    "\n",
    "        if out.loss is not None and torch.isfinite(out.loss):\n",
    "            loss_sum += float(out.loss)\n",
    "            nloss += 1\n",
    "\n",
    "        del logits, labels, out\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    acc = correct / max(total, 1)\n",
    "    avg_loss = (loss_sum / nloss) if nloss > 0 else float(\"nan\")\n",
    "    return {\"eval_accuracy\": acc, \"eval_loss_stream\": avg_loss}\n",
    "\n",
    "metrics_stream = manual_eval_accuracy(full_trainer.model, val_ds, collator, batch_size=2)\n",
    "print(metrics_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89275743-3993-4583-8514-85cf0f8e1aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch, pandas as pd\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def predict_letters(model, ds, collator, batch_size=4):\n",
    "#     model.eval()\n",
    "#     loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
    "#     device = next(model.parameters()).device\n",
    "#     preds = []\n",
    "#     for batch in loader:\n",
    "#         inputs = {k: v.to(device) for k,v in batch.items() if k in [\"input_ids\",\"attention_mask\"]}\n",
    "#         out = model.generate(**inputs, max_new_tokens=2)\n",
    "#         for i in range(out.size(0)):\n",
    "#             # strip the prompt tokens; take only the generated tokens\n",
    "#             prompt_len = (batch[\"input_ids\"][i] != tok.pad_token_id).sum().item()\n",
    "#             gen = tok.decode(out[i][prompt_len:], skip_special_tokens=True).strip()\n",
    "#             letter = next((c for c in gen if c in LETTERS), None)\n",
    "#             preds.append(letter or \"\")\n",
    "#     return preds\n",
    "\n",
    "# test_preds = predict_letters(full_trainer.model, test_ds, collator, batch_size=4)\n",
    "\n",
    "# # Save with the original test ids so you can align externally if needed\n",
    "# test_ids = [csqa[\"test\"][i][\"id\"] for i in range(len(csqa[\"test\"]))]\n",
    "# df = pd.DataFrame({\"id\": test_ids, \"pred\": test_preds})\n",
    "# df.to_csv(\"csqa_test_predictions.csv\", index=False)\n",
    "# print(\"Wrote csqa_test_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2bee1-e41d-42e1-920d-3137ae7d089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./csqa_gemma1b_full/adapter\"\n",
    "\n",
    "# Save only the trainable LoRA layers\n",
    "full_trainer.model.save_pretrained(save_dir)\n",
    "\n",
    "# Save tokenizer too (same tok you used for CSQA)\n",
    "tok.save_pretrained(save_dir)\n",
    "\n",
    "print(\"Saved adapter + tokenizer to:\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a197ab-13ed-4fc7-ada0-b1b2b211b863",
   "metadata": {},
   "source": [
    "## Llama 3B\n",
    "\n",
    "Trainable: 24,313,856 / 3,237,063,680 (0.751%)\n",
    "\n",
    "TrainOutput(global_step=2436, training_loss=0.3055989429085517, metrics={'train_runtime': 1158.7058, 'train_samples_per_second': 16.814, 'train_steps_per_second': 2.102, 'total_flos': 2.8936654637654016e+16, 'train_loss': 0.3055989429085517, 'epoch': 2.0})\n",
    "\n",
    "{'eval_accuracy': 0.8230958230958231, 'eval_loss_stream': 0.30398253511366224}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521fa43-5869-456b-a528-48ed9686c575",
   "metadata": {},
   "source": [
    "## Llama 1B\n",
    "\n",
    "Trainable: 11,272,192 / 1,247,086,592 (0.904%)\n",
    "\n",
    "TrainOutput(global_step=2436, training_loss=0.40509649430981215, metrics={'train_runtime': 670.9873, 'train_samples_per_second': 29.035, 'train_steps_per_second': 3.63, 'total_flos': 1.0019401622765568e+16, 'train_loss': 0.40509649430981215, 'epoch': 2.0})\n",
    "\n",
    "{'eval_accuracy': 0.7592137592137592, 'eval_loss_stream': 0.4115677761792704}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad68e5-e9ed-4d23-bab2-9869260c1f0a",
   "metadata": {},
   "source": [
    "## Qwen 3B\n",
    "\n",
    "Trainable: 29,933,568 / 3,115,872,256 (0.961%)\n",
    "\n",
    "TrainOutput(global_step=2436, training_loss=0.3242768377114595, metrics={'train_runtime': 1490.2816, 'train_samples_per_second': 13.073, 'train_steps_per_second': 1.635, 'total_flos': 2.822591263806259e+16, 'train_loss': 0.3242768377114595, 'epoch': 2.0})\n",
    "\n",
    "{'eval_accuracy': 0.8378378378378378, 'eval_loss_stream': 0.28810480255784116}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaac331-3135-4396-928b-661122e5c8bf",
   "metadata": {},
   "source": [
    "## Qwen 1.5B\n",
    "\n",
    "Trainable: 18,464,768 / 1,562,179,072 (1.182%)\n",
    "\n",
    "TrainOutput(global_step=2436, training_loss=0.3098135355658132, metrics={'train_runtime': 1149.0526, 'train_samples_per_second': 16.955, 'train_steps_per_second': 2.12, 'total_flos': 1.3372783705995264e+16, 'train_loss': 0.3098135355658132, 'epoch': 2.0})\n",
    "\n",
    "{'eval_accuracy': 0.7993447993447993, 'eval_loss_stream': 0.3620616310815679}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3674f8-d754-4ad1-b635-62d0971a57b3",
   "metadata": {},
   "source": [
    "## Qwen 0.5B\n",
    "\n",
    "Trainable: 8,798,208 / 502,830,976 (1.750%)\n",
    "\n",
    "TrainOutput(global_step=2436, training_loss=0.4317081522667545, metrics={'train_runtime': 978.8359, 'train_samples_per_second': 19.903, 'train_steps_per_second': 2.489, 'total_flos': 3690345224148480.0, 'train_loss': 0.4317081522667545, 'epoch': 2.0})\n",
    "\n",
    "{'eval_accuracy': 0.6764946764946765, 'eval_loss_stream': 0.5564539690013082}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e8516-bcd0-4ec0-95cd-95a4ca6b8589",
   "metadata": {},
   "source": [
    "## Gemma 4b\n",
    "\n",
    "Trainable: 32,788,480 / 4,332,867,952 (0.757%)\n",
    "\n",
    "TrainOutput(global_step=2436, training_loss=0.596402333092024, metrics={'train_runtime': 1788.0983, 'train_samples_per_second': 10.895, 'train_steps_per_second': 1.362, 'total_flos': 3.884991434613744e+16, 'train_loss': 0.596402333092024, 'epoch': 2.0})\n",
    "\n",
    "\n",
    "{'eval_accuracy': 0.8091728091728092, 'eval_loss_stream': 0.34752246979196766}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e5e58b-a3b2-4d13-9a26-75ea8adca6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS701 env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
